{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotImageClassification, AutoModel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from loadimg import load_img\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.tools import art_cropper\n",
    "from src.transformations import image_transforms_no_tensor\n",
    "from torch.nn.functional import cosine_similarity\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "be98df15032b099"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "dataset = load_dataset(\"imagefolder\", data_dir=\"./data\")",
   "id": "927201875fd0ad55"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "dataset.push_to_hub(f\"HichTala/yugioh\")",
   "id": "7753932abd4cc39"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "dataset = load_dataset(\"HichTala/yugioh\")",
   "id": "69d3e5850693a15b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"google/vit-huge-patch14-224-in21k\")\n",
    "model = AutoModel.from_pretrained(\"google/vit-huge-patch14-224-in21k\", device_map=device)\n"
   ],
   "id": "2b2388fc225faef4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def embed(batch):\n",
    "    pixel_values = processor(images=batch[\"image\"], return_tensors=\"pt\")['pixel_values']\n",
    "    pixel_values = pixel_values.to(device)\n",
    "    img_emb = model.get_image_features(pixel_values)\n",
    "    batch[\"embeddings\"] = img_emb\n",
    "    return batch\n",
    "\n",
    "\n",
    "embedded_dataset = dataset.map(embed, batched=True, batch_size=16)"
   ],
   "id": "af521d5a8fda5312"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "embedded_dataset.push_to_hub(\"HichTala/yugioh-embeddings\")\n",
   "id": "e80186c34df28096"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "dataset = load_dataset(\"HichTala/yugioh-embeddings\", split=\"train\")",
   "id": "7cfaea6825c09396"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "dataset = dataset.add_faiss_index(\"embeddings\")",
   "id": "b8e0596afc9beb25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def search(query: str, k: int = 4):\n",
    "    \"\"\"a function that embeds a new image and returns the most probable results\"\"\"\n",
    "\n",
    "    pixel_values = processor(images=query, return_tensors=\"pt\")['pixel_values']  # embed new image\n",
    "    pixel_values = pixel_values.to(device)\n",
    "    img_emb = model(pixel_values).pooler_output[0] # because it's a single element\n",
    "    img_emb = img_emb.cpu().detach().numpy()  # convert to numpy because the datasets library does not support torch vectors\n",
    "\n",
    "    scores, retrieved_examples = dataset.get_nearest_examples(  # retrieve results\n",
    "        \"embeddings\", img_emb,  # compare our new embedded image with the dataset embeddings\n",
    "        k=k  # get only top k results\n",
    "    )\n",
    "\n",
    "    return retrieved_examples"
   ],
   "id": "e7304646ebcf379b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def image_resize(image, width = None, height = None, inter = cv2.INTER_AREA):\n",
    "    # initialize the dimensions of the image to be resized and\n",
    "    # grab the image size\n",
    "    dim = None\n",
    "    (h, w) = image.shape[:2]\n",
    "    # if both the width and height are None, then return the\n",
    "    # original image\n",
    "    if width is None and height is None:\n",
    "        return image\n",
    "    # check to see if the width is None\n",
    "    if width is None:\n",
    "        # calculate the ratio of the height and construct the\n",
    "        # dimensions\n",
    "        r = height / float(h)\n",
    "        dim = (int(w * r), height)\n",
    "    # otherwise, the height is None\n",
    "    else:\n",
    "        # calculate the ratio of the width and construct the\n",
    "        # dimensions\n",
    "        r = width / float(w)\n",
    "        dim = (width, int(h * r))\n",
    "    # resize the image\n",
    "    resized = cv2.resize(image, dim, interpolation = inter)\n",
    "    # return the resized image\n",
    "    return resized"
   ],
   "id": "e340cce27d2d4046"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "query = \"queries/rh_0.jpg\"",
   "id": "62ab032cf55a53ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "image = cv2.imread(query)\n",
    "image = image_resize(image, width=536)\n",
    "\n",
    "# image = load_img(query).resize(((536, 782)))\n",
    "image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "image"
   ],
   "id": "a03300acec0163c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "image = load_img(query)\n",
    "image"
   ],
   "id": "c8442963b757b233"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "image = load_img(query)\n",
    "image = art_cropper(image)\n",
    "image"
   ],
   "id": "50b17c2c8258a1a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "nb_rows = 2\n",
    "nb_image = nb_rows ** 2\n"
   ],
   "id": "b5bb9f806e05b578"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "retrieved_examples = search(image, nb_image)\n",
    "f, axarr = plt.subplots(nb_rows, nb_rows)\n",
    "for index in range(nb_image):\n",
    "    i, j = index // nb_rows, index % nb_rows\n",
    "    # axarr[i,j].set_title(retrieved_examples[\"text\"][index])\n",
    "    axarr[i, j].imshow(retrieved_examples[\"image\"][index])\n",
    "    axarr[i, j].axis('off')\n",
    "plt.show()"
   ],
   "id": "ded349bc12737871"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'",
   "id": "ffd23dca18d67b48"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def infer(image):\n",
    "  inputs = processor(image, return_tensors=\"pt\").to(DEVICE)\n",
    "  outputs = model(**inputs)\n",
    "  return outputs.pooler_output"
   ],
   "id": "47e099faeae28896"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "dataset[0]",
   "id": "261d9556c21db7e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "query1 = \"queries/rh_0.jpg\"\n",
    "query2 = \"queries/rh_0_enhanced.png\"\n",
    "query3 = \"queries/image.png\""
   ],
   "id": "2910bd86214b6a0b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "image_real = Image.open(query1).crop((13, 30, 81, 97))\n",
    "image_2 = Image.open(query2).crop((25, 58, 161, 194))\n",
    "image_3 = Image.open(query3).crop((53, 118, 322, 385))"
   ],
   "id": "f92159cfa36ee470"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cv2.imshow(\"\", cv2.imread(query3))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ],
   "id": "de4cddbfa2ad5e10"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "image_real = image_transforms_no_tensor(image_real)\n",
    "image_2 = image_transforms_no_tensor(image_2)\n",
    "image_3 = image_transforms_no_tensor(image_3)"
   ],
   "id": "e8f539287a8bbaf3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "embed_real = infer(image_real)\n",
    "embed_2 = infer(image_2)\n",
    "embed_3 = infer(image_3)"
   ],
   "id": "3a41b00fede4a47c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cv2.imshow(\"\", cv2.imread(query1))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ],
   "id": "bf56a64b2b5b8020"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "similarities = {}\n",
    "similarities2 = {}\n",
    "similarities3 = {}\n",
    "with tqdm(total=13701, desc=\"Augmenting Dataset\", colour='cyan') as pbar:\n",
    "    for subdir, dirs, files in os.walk(\"./data\"):\n",
    "        for file in files:\n",
    "            pbar.update(1)\n",
    "            image_gen = Image.open(os.path.join(subdir, file))\n",
    "            embed_gen = infer(image_gen)\n",
    "            similarities[subdir] = cosine_similarity(embed_real, embed_gen, dim=1).cpu().detach().item()\n",
    "            similarities2[subdir] = cosine_similarity(embed_2, embed_gen, dim=1).cpu().detach().item()\n",
    "            similarities[subdir] = cosine_similarity(embed_3, embed_gen, dim=1).cpu().detach().item()"
   ],
   "id": "29b179a0ceb34e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "similarity_score = cosine_similarity(embed_real, embed_gen, dim=1)\n",
    "print(similarity_score)"
   ],
   "id": "3883d923a61dbe71"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "similarity_score = cosine_similarity(embed_real, embed_gen, dim=1)\n",
    "print(similarity_score)"
   ],
   "id": "4a25b21e884965b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "similarities_sorted = sorted(similarities.items(), key=lambda x: x[1], reverse=True)",
   "id": "af4b574bde693f1a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "similarities = {k: v for k, v in similarities}",
   "id": "4d3a47c41ed5124f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "similarities2_sorted = sorted(similarities2.items(), key=lambda x: x[1], reverse=True)\n",
    "similarities3_sorted = sorted(similarities3.items(), key=lambda x: x[1], reverse=True)\n"
   ],
   "id": "8deabb746816b60b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "similarities",
   "id": "b171fce18d2a2ecd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "similarities['./data/Harpie-Queen-0-75064463']",
   "id": "285f9c6fae12445b"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
